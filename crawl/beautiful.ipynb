{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup \n",
    "- 파서(html, xml 의 형태로 내려온 데이터를 원하는 요소만 찾기 위해 필요)\n",
    "- requests + bs4 : 이 조합으로 주로 크롤링\n",
    "- 파서 종류\n",
    "    - html.parser (두번째 속도)\n",
    "    - lxml (속도가 가장 빠름) : 설치 필요 pip install lxml\n",
    "    - html5lib (가장 느림)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>의대증원 오늘 확정…전공의·의대생 복귀 가능성 더 멀어졌다</title>\n",
      "<h3 class=\"tit_view\" data-translation=\"true\">의대증원 오늘 확정…전공의·의대생 복귀 가능성 더 멀어졌다</h3>\n",
      "의대증원 오늘 확정…전공의·의대생 복귀 가능성 더 멀어졌다\n",
      "{'class': ['tit_view'], 'data-translation': 'true'}\n"
     ]
    }
   ],
   "source": [
    "url = \"https://v.daum.net/v/20240524104849821\"\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url)\n",
    "    # print(r.text)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    # print(soup)\n",
    "\n",
    "    # 요소 접근\n",
    "    # 태그명 사용\n",
    "    print(soup.title)\n",
    "    print(soup.h3)\n",
    "    # get_text() : 태그의 텍스트 추출\n",
    "    print(soup.title.get_text())\n",
    "    # attrs : 태그 속성 추출\n",
    "    print(soup.h3.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title <title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      "title content \n",
      "   The Dormouse's story\n",
      "  \n",
      "title content \n",
      "   The Dormouse's story\n",
      "  \n",
      "title parent <head>\n",
      "<title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      "</head>\n",
      "==========\n",
      "p <p class=\"title\">\n",
      "<b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "</p>\n",
      "p The Dormouse's story\n",
      "p {'class': ['title']}\n",
      "p ['title']\n",
      "p <b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "p The Dormouse's story\n",
      "p The Dormouse's story\n"
     ]
    }
   ],
   "source": [
    "url = \"./story.html\"\n",
    "\n",
    "with open(url, \"r\") as f:\n",
    "    r = f.read()\n",
    "    # print(r.text)\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    # print(soup)\n",
    "    # title 태그 가져오기\n",
    "    title = soup.title\n",
    "    print(f\"title {title}\")\n",
    "    print(f\"title content {title.get_text()}\")\n",
    "    print(f\"title content {title.string}\")\n",
    "    print(f\"title parent {title.parent}\")\n",
    "    print(\"=\"*10)\n",
    "    # p 태그 가져오기\n",
    "    p1 = soup.p\n",
    "    print(f\"p {p1}\")\n",
    "    print(f\"p {p1.get_text().strip()}\")\n",
    "    print(f\"p {p1.attrs}\")\n",
    "    print(f\"p {p1[\"class\"]}\")\n",
    "    # b 태그 가져오기\n",
    "    b1 = soup.b\n",
    "    print(f\"p {b1}\")\n",
    "    print(f\"p {b1.get_text().strip()}\")\n",
    "    print(f\"p {b1.string.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p2 <p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "    Elsie\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      "    Lacie\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
      "    Tillie\n",
      "   </a>\n",
      "   ; and they lived at the bottom of a well.\n",
      "  </p>\n",
      "p2 Once upon a time there were three little sisters; and their names were\n",
      "   \n",
      "    Elsie\n",
      "   \n",
      "   ,\n",
      "   \n",
      "    Lacie\n",
      "   \n",
      "   and\n",
      "   \n",
      "    Tillie\n",
      "   \n",
      "   ; and they lived at the bottom of a well.\n",
      "p2 {'class': ['story']}\n",
      "p2 ['story']\n"
     ]
    }
   ],
   "source": [
    "# 문서의 구조를 이용한 요소 찾기\n",
    "# parent, children, next_sibling.....\n",
    "\n",
    "url = \"./story.html\"\n",
    "\n",
    "with open(url, \"r\") as f:\n",
    "    r = f.read()\n",
    "    # print(r.text)\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    # body = soup.body\n",
    "    # print(f\"body children {body.children}\")\n",
    "    # for child in body.children:\n",
    "    #     print(child)\n",
    "\n",
    "    # 첫번째 p 요소 찾기\n",
    "    p1 = soup.p\n",
    "    p2 = p1.find_next_sibling(\"p\")\n",
    "    print(f\"p2 {p2}\")\n",
    "    print(f\"p2 {p2.get_text().strip()}\")\n",
    "    # print(f\"p2 {p2.string.strip()}\")\n",
    "    print(f\"p2 {p2.attrs}\")\n",
    "    print(f\"p2 {p2[\"class\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head>\n",
      "<title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      "</head>\n",
      "<p class=\"title\">\n",
      "<b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "</p>\n",
      "<p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "    Elsie\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      "    Lacie\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
      "    Tillie\n",
      "   </a>\n",
      "   ; and they lived at the bottom of a well.\n",
      "  </p>\n",
      "<p class=\"story\">\n",
      "   ...\n",
      "  </p>\n",
      "====================\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "    Elsie\n",
      "   </a>\n",
      "http://example.com/tillie\n",
      "====================\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "    Elsie\n",
      "   </a>\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      "    Lacie\n",
      "   </a>\n"
     ]
    }
   ],
   "source": [
    "# find() : 조건을 만족하는 요소 한개 찾기\n",
    "# find_all(): 조건을 만족하는 요소 모두 찾기\n",
    "\n",
    "url = \"./story.html\"\n",
    "\n",
    "with open(url, \"r\") as f:\n",
    "    r = f.read()\n",
    "    # print(r.text)\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    head = soup.find(\"head\")\n",
    "    print(head)\n",
    "\n",
    "    # p1 = soup.find(\"p\")\n",
    "    # print(p1)\n",
    "\n",
    "    p1 = soup.find(\"p\", attrs={\"class\":\"title\"})\n",
    "    print(p1)\n",
    "\n",
    "    # p2 = soup.find(\"p\", attrs={\"class\":\"story\"})\n",
    "    p2 = soup.find(\"p\", class_=\"story\")\n",
    "    print(p2)\n",
    "\n",
    "    p_all = soup.find_all(\"p\", class_=\"story\")\n",
    "    # print(p_all)\n",
    "    print(p_all[1])\n",
    "\n",
    "    print(\"=\"*20)\n",
    "    # a1 = soup.find(\"a\", attrs={\"id\":\"link1\"})\n",
    "    a1 = soup.find(\"a\", id =\"link1\")\n",
    "    print(a1)\n",
    "\n",
    "    a3 = soup.find(\"a\", id=\"link3\")\n",
    "    print(a3[\"href\"])\n",
    "\n",
    "    print(\"=\"*20)\n",
    "    a_tags = soup.find_all(\"a\", limit=2)\n",
    "    for ele in a_tags:\n",
    "        print(ele)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elsie', 'Lacie', 'Tillie']\n",
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
     ]
    }
   ],
   "source": [
    "url = \"./story.html\"\n",
    "\n",
    "with open(url, \"r\") as f:\n",
    "    r = f.read()\n",
    "    # print(r.text)\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    # link1 = soup.find_all(string=\"Elsie\")\n",
    "    link1 = soup.find_all(string=[\"Elsie\",\"Lacie\",\"Tillie\"])\n",
    "    link2 = soup.find_all(\"a\",string=[\"Elsie\",\"Lacie\",\"Tillie\"])\n",
    "    print(link1)\n",
    "    print(link2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna SchererEmpress Marya\n",
      "FedorovnaPrince Vasili KuraginAnna PavlovnaSt. Petersburgthe princeAnna PavlovnaAnna Pavlovnathe princethe princethe princePrince VasiliAnna PavlovnaAnna Pavlovnathe princeWintzingerodeKing of Prussiale Vicomte de MortemartMontmorencysRohansAbbe Moriothe Emperorthe princePrince VasiliDowager Empress Marya Fedorovnathe baronAnna Pavlovnathe Empressthe EmpressAnna Pavlovna'sHer MajestyBaron\n",
      "FunkeThe princeAnna\n",
      "Pavlovnathe EmpressThe princeAnatolethe princeThe princeAnna\n",
      "PavlovnaAnna PavlovnaWell, Prince, so Genoa and Lucca are now just family estates of the\n",
      "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
      "if you still try to defend the infamies and horrors perpetrated by\n",
      "that Antichrist- I really believe he is Antichrist- I will have\n",
      "nothing more to do with you and you are no longer my friend, no longer\n",
      "my 'faithful slave,' as you call yourself! But how do you do? I see\n",
      "I have frightened you- sit down and tell me all the news.\n",
      "If you have nothing better to do, Count [or Prince], and if the\n",
      "prospect of spending an evening with a poor invalid is not too\n",
      "terrible, I shall be very charmed to see you tonight between 7 and 10-\n",
      "Annette Scherer.\n",
      "Heavens! what a virulent attack!\n",
      "First of all, dear friend, tell me how you are. Set your friend's\n",
      "mind at rest,\n",
      "Can one be well while suffering morally? Can one be calm in times\n",
      "like these if one has any feeling?\n",
      "You are\n",
      "staying the whole evening, I hope?\n",
      "And the fete at the English ambassador's? Today is Wednesday. I\n",
      "must put in an appearance there,\n",
      "My daughter is\n",
      "coming for me to take me there.\n",
      "I thought today's fete had been canceled. I confess all these\n",
      "festivities and fireworks are becoming wearisome.\n",
      "If they had known that you wished it, the entertainment would\n",
      "have been put off,\n",
      "Don't tease! Well, and what has been decided about Novosiltsev's\n",
      "dispatch? You know everything.\n",
      "What can one say about it?\n",
      "What has been decided? They have decided that\n",
      "Buonaparte has burnt his boats, and I believe that we are ready to\n",
      "burn ours.\n",
      "Oh, don't speak to me of Austria. Perhaps I don't understand\n",
      "things, but Austria never has wished, and does not wish, for war.\n",
      "She is betraying us! Russia alone must save Europe. Our gracious\n",
      "sovereign recognizes his high vocation and will be true to it. That is\n",
      "the one thing I have faith in! Our good and wonderful sovereign has to\n",
      "perform the noblest role on earth, and he is so virtuous and noble\n",
      "that God will not forsake him. He will fulfill his vocation and\n",
      "crush the hydra of revolution, which has become more terrible than\n",
      "ever in the person of this murderer and villain! We alone must\n",
      "avenge the blood of the just one.... Whom, I ask you, can we rely\n",
      "on?... England with her commercial spirit will not and cannot\n",
      "understand the Emperor Alexander's loftiness of soul. She has\n",
      "refused to evacuate Malta. She wanted to find, and still seeks, some\n",
      "secret motive in our actions. What answer did Novosiltsev get? None.\n",
      "The English have not understood and cannot understand the\n",
      "self-abnegation of our Emperor who wants nothing for himself, but only\n",
      "desires the good of mankind. And what have they promised? Nothing! And\n",
      "what little they have promised they will not perform! Prussia has\n",
      "always declared that Buonaparte is invincible, and that all Europe\n",
      "is powerless before him.... And I don't believe a word that Hardenburg\n",
      "says, or Haugwitz either. This famous Prussian neutrality is just a\n",
      "trap. I have faith only in God and the lofty destiny of our adored\n",
      "monarch. He will save Europe!\n",
      "I think,\n",
      "None\n",
      "In a moment. A propos,\n",
      "None\n",
      "I shall be delighted to meet them,\n",
      "But tell me,\n",
      "is it true that the Dowager Empress wants Baron Funke\n",
      "to be appointed first secretary at Vienna? The baron by all accounts\n",
      "is a poor creature.\n",
      "Baron Funke has been recommended to the Dowager Empress by her\n",
      "sister,\n",
      "Now about your family. Do you know that since your daughter came\n",
      "out everyone has been enraptured by her? They say she is amazingly\n",
      "beautiful.\n",
      "I often think,\n",
      "None\n",
      "Two such charming children. And really you appreciate\n",
      "them less than anyone, and so you don't deserve to have them.\n",
      "I can't help it,\n",
      "Lavater would have said I\n",
      "lack the bump of paternity.\n",
      "Don't joke; I mean to have a serious talk with you. Do you know I\n",
      "am dissatisfied with your younger son? Between ourselves\n",
      "he was mentioned at Her\n",
      "Majesty's and you were pitied....\n",
      "What would you have me do?\n",
      "You know I did all\n",
      "a father could for their education, and they have both turned out\n",
      "fools. Hippolyte is at least a quiet fool, but Anatole is an active\n",
      "one. That is the only difference between them.\n",
      "And why are children born to such men as you? If you were not a\n",
      "father there would be nothing I could reproach you with,\n",
      "I am your faithful slave and to you alone I can confess that my\n",
      "children are the bane of my life. It is the cross I have to bear. That\n",
      "is how I explain it to myself. It can't be helped!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://pythonscraping.com/pages/warandpeace.html\"\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url)    \n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    # 등장인물 출력\n",
    "    # names = soup.find_all(\"span\",attrs={\"class\":\"green\"})\n",
    "    names = soup.find_all(\"span\", class_=\"green\")\n",
    "    for name in names:\n",
    "        print(name.string, end=\"\")\n",
    "\n",
    "    # 대사 출력 \n",
    "    dialoues = soup.find_all(\"span\",class_=\"red\")\n",
    "    for d in dialoues:\n",
    "        print(d.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목 : 의대증원 오늘 확정…전공의·의대생 복귀 가능성 더 멀어졌다\n",
      "작성자 : 천선휴 기자\n",
      "작성날짜와 시간 : 2024. 5. 24. 10:48\n",
      "첫번째 문단 (서울=뉴스1) 천선휴 기자 = 2025학년도 의과대학 정원이 24일 확정된다. 1999년 이후 27년 만의 의대정원 증원이다. \n",
      "[<p dmcf-pid=\"06iSrs1mpj\" dmcf-ptype=\"general\">(서울=뉴스1) 천선휴 기자 = 2025학년도 의과대학 정원이 24일 확정된다. 1999년 이후 27년 만의 의대정원 증원이다. </p>, <p dmcf-pid=\"pPnvmOts0N\" dmcf-ptype=\"general\">하지만 이로써 학교를 떠나 있는 의대생들과 병원을 이탈한 전공의들이 돌아올 가능성도 더 멀어졌다. </p>, <p dmcf-pid=\"UOW2uze7ua\" dmcf-ptype=\"general\">한국대학교육협의회는 24일 오후 제2차 대학입학전형위원회를 열고 의대 증원안이 포함된 대입전형 시행계획 변경안을 심의·확정한다. 이에 대한 심의 결과는 30일에, 대학별 모집요강은 31일에 발표된다. 이달 말이면 의대 증원 절차가 모두 마무리되는 것이다.</p>, <p dmcf-pid=\"uIYV7qdz7g\" dmcf-ptype=\"general\">일부 국립대에서는 의대 정원 증원을 확정하기 위해 필요한 학칙 개정이 부결되는 등 학내 갈등이 이어지고 있지만, 그럼에도 2025학년도 대입 전형과 모집정원은 그대로 확정된다는 게 교육부의 설명이다.</p>, <p dmcf-pid=\"7CGfzBJquo\" dmcf-ptype=\"general\">이로써 내년 전국 40개 의과대학은 전년보다 1509명이 늘어난 4567명의 신입생을 뽑게 된다. 동시에 '원점 재검토'를 복귀의 전제로 주장해오던 전공의와 의대생들이 돌아올 명분도 사라졌다. </p>, <p dmcf-pid=\"zwSB3pQ03L\" dmcf-ptype=\"general\">전공의들이 끝내 돌아오지 않으면 당장 내년에 전문의 2910명이 배출되지 못한다. 이들 중엔 필수의료과에서 수련하던 전공의 1385명도 포함돼 있다. 가뜩이나 부족한 필수과 전문의들마저 1400여 명이 배출되지 않는다는 것이다. </p>, <p dmcf-pid=\"qrvb0Uxp3n\" dmcf-ptype=\"general\">전문의 배출이 되지 않을 경우 군의관, 공중보건의 모집에도 영향을 미칠 수밖에 없다.</p>, <p dmcf-pid=\"BmTKpuMUFi\" dmcf-ptype=\"general\">다만 의료계에선 전문의 시험만 남겨둔 레지던트 4년 차(3년제 진료과목은 3년 차) 전공의들은 복귀해 시험을 치를 것이라는 전망도 내놓고 있다. 하지만 대세를 뒤흔들 만큼의 수가 아닌 극소수에 불과할 것으로 보고 있다. </p>, <p dmcf-pid=\"KIYV7qdz3d\" dmcf-ptype=\"general\">의대생들이 돌아올 확률은 더욱 적어 보인다. 전공의들은 가정을 꾸리거나 외벌이인 경우가 많아 생계를 이유로 돌아오길 고민하는 경우도 일부 있다고는 하지만 의대생들의 의지는 확고하다. 현재 의대생 1만8348명 중 99.7%가 학교에 나오지 않고 있다. </p>, <p dmcf-pid=\"9CGfzBJqpe\" dmcf-ptype=\"general\">이들이 이 상태로 유급에 처해질 경우 현재 1학년인 3058명의 학생들과 내년 신입생 4567명까지 총 7625이 함께 1학년 수업을 들어야 한다. 각 대학의 현재 시설과 인력으로는 감당할 수 없는 인원이다. </p>, <p dmcf-pid=\"2cIu5t41pR\" dmcf-ptype=\"general\">또 의대생 졸업자가 나오지 않으면 병원들은 인턴도 뽑지 못하게 된다. </p>, <p dmcf-pid=\"VkC71F8tFM\" dmcf-ptype=\"general\">문제는 각 대학의 교육 여건이 미진할 경우 한국의학교육평가원이 진행하는 평가에서 인증을 받지 못하게 돼 서남대 의대처럼 폐교를 하게 되는 일이 벌어질 수도 있다는 점이다. </p>, <p dmcf-pid=\"fEhzt36F3x\" dmcf-ptype=\"general\">한 의과대학 교수는 \"엄청나게 늘어난 저 신입생들을 교육시킬 여력이 안된다는 건 너무나 자명한 사실\"이라며 \"2025학년 의예과 1학년인 7625명은 6년간 학교를 같이 다녀야 하는데 향후 6년간 큰 부담을 줄 것\"이라고 말했다. </p>, <p dmcf-pid=\"4QLTsIFOUQ\" dmcf-ptype=\"general\">또 이들이 졸업해 인턴을 지원할 때도 문제가 된다. 이 교수는 \"내년부터 늘어난 의대 정원이 졸업하는 2031년 7625명이 인턴을 지원하게 되는 것인데 병원이 얼마나 인턴 선발의 여력이 있을지 모르겠다\"며 \"결국 의대는 졸업하지만 인턴, 레지던트 과정을 밟을 수 없는 의사들은 어떻게 되는 것인가\"라고 우려했다. </p>, <p dmcf-pid=\"8xoyOC3IpP\" dmcf-ptype=\"general\">그러면서 \"대학병원들이 아직도 전공의가 들어오길 바라고, 전공의로 운영되는 진료시스템에 대해 미련을 가지고 있는데 이제 그런 상상은 2036년부터나 가능할 것\"이라고 말했다.</p>, <p dmcf-pid=\"6MgWIh0Cz6\" dmcf-ptype=\"general\">정부는 비상진료체계 강화 대책을 잇따라 내놓으며 전공의 이탈에 따른 의료공백을 메우고 있지만 현실적으로 역부족인 상황이다. </p>, <p dmcf-pid=\"Pq8DgNva38\" dmcf-ptype=\"general\">전공의들이 떠난 지 석달째 비상진료 체계를 운영하면서 공중보건의, 군의관을 파견하고 있지만 547명에 불과해 전공의 1만3000여 명의 자리를 메우기엔 턱없이 부족하다. </p>, <p dmcf-pid=\"QB6wajTNU4\" dmcf-ptype=\"general\">또 다른 대책으로 시니어 의사 채용 방안을 내놓고 지난달 16일 시니어 의사 지원센터를 개소했지만 문의 전화는 8건뿐인 데다 예산 확정 등도 되지 않아 초기 세팅 단계에 머무르고 있다.</p>, <p dmcf-pid=\"xbPrNAyj0f\" dmcf-ptype=\"general\">정부가 내세우고 있는 '전문의 중심 병원'도 당장 의료공백을 메울 수 없는 대책이긴 매한가지다. 병원 관계자들은 상급종합병원의 경우 전공의 비율이 40%에 달했는데 이들을 대신해 전문의를 채용하는 것은 불가능하다고 입을 모은다. </p>, <p dmcf-pid=\"y57nQMDxzV\" dmcf-ptype=\"general\">이 같은 문제를 정부도 모르고 있는 것은 아니다. 조규홍 복지부 장관은 지난 22일 기자간담회에서 \"군의관, 공보의도 투입하고 간호사들의 진료지원도 확대하는 등 대책을 마련하고 있는데 이게 지속가능하진 않다\"며 \"하루빨리 전공의 돌아와서 자리를 메꿔주면 좋겠다\"고 말했다. </p>, <p dmcf-pid=\"W1zLxRwMp2\" dmcf-ptype=\"general\">그러면서 \"의료개혁 특별위원회를 빨리 가동하고 논의를 해서 의료개혁을 완성해가는 것이 정부의 대책\"이라고 덧붙였다. </p>, <p dmcf-pid=\"YtqoMerRp9\" dmcf-ptype=\"general\">이에 정부는 이날 의개특위 산하 4개 전문위원회 중 의료인력 전문위원회의 첫 회의를 열고 전공의 연속 근무 시간 단축 등을 논의할 계획이다. 이날로 4개 전문위원회의 1차 회의가 마무리되는 것이다. </p>, <p dmcf-pid=\"GFBgRdme7K\" dmcf-ptype=\"general\">이 전문위원회들은 앞으로 격주 회의를 열고 개혁 과제를 구체화해 나갈 방침이다.</p>, <p dmcf-pid=\"HHpd8Pc6pb\" dmcf-ptype=\"general\">sssunhue@news1.kr </p>]\n"
     ]
    }
   ],
   "source": [
    "url = \"https://v.daum.net/v/20240524104849821\"\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url)\n",
    "    # print(r.text)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    # 뉴스 제목 \n",
    "    title = soup.h3\n",
    "    print(f\"제목 : {title.text}\")\n",
    "\n",
    "    # 작성자 \n",
    "    writer = soup.find(\"span\", class_=\"txt_info\")\n",
    "    print(f\"작성자 : {writer.string}\")\n",
    "\n",
    "    # 작성날짜와 시간\n",
    "    num_date = soup.find(\"span\", class_=\"num_date\")\n",
    "    print(f\"작성날짜와 시간 : {num_date.string}\")\n",
    "\n",
    "    # 첫번째 문단 가져오기\n",
    "    para = soup.find(\"p\", attrs={\"dmcf-ptype\":\"general\"})\n",
    "    print(f\"첫번째 문단 {para.text}\")\n",
    "\n",
    "    # 전체 본문 내용 가져오기\n",
    "    paras = soup.find_all(\"p\", attrs={\"dmcf-ptype\":\"general\"})\n",
    "    # print(paras)\n",
    "    for p in paras:\n",
    "        print(p.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b> The Dormouse's story </b>\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n"
     ]
    }
   ],
   "source": [
    "# css select 사용\n",
    "# select() : 전체 요소 / select_one()\n",
    "\n",
    "url = \"./story.html\"\n",
    "\n",
    "with open(url, \"r\") as f:\n",
    "    r = f.read()\n",
    "    # print(r.text)\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    title = soup.select_one(\"p.title > b\")\n",
    "    print(title)\n",
    "\n",
    "    link1 = soup.select_one(\"#link1\")\n",
    "    print(link1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "Elsie\n",
      "Elsie\n",
      "Elsie\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n",
      "Lacie\n",
      "Lacie\n",
      "Lacie\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
      "Tillie\n",
      "Tillie\n",
      "Tillie\n"
     ]
    }
   ],
   "source": [
    "url = \"./story.html\"\n",
    "\n",
    "with open(url, \"r\") as f:\n",
    "    r = f.read()\n",
    "    # print(r.text)\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    stories = soup.select(\"p.story > a\")\n",
    "    # print(stories)\n",
    "\n",
    "    for story in stories:\n",
    "        print(story)\n",
    "        print(story.text)\n",
    "        print(story.string)\n",
    "        print(story.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "==== Elsie\n",
      "==== <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n",
      "==== Lacie\n",
      "==== <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
      "==== Tillie\n",
      "===>  <p class=\"story\">...</p>\n",
      "===>  ...\n"
     ]
    }
   ],
   "source": [
    "url = \"./story.html\"\n",
    "\n",
    "with open(url, \"r\") as f:\n",
    "    r = f.read()\n",
    "    # print(r.text)\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    stories = soup.select(\"p.story\")\n",
    "    # print(stories)\n",
    "\n",
    "    for story in stories:\n",
    "        temp = story.find_all(\"a\")\n",
    "\n",
    "        if temp:\n",
    "            for v in temp:\n",
    "                print(\"====\", v)\n",
    "                print(\"====\", v.string)\n",
    "        else:\n",
    "            print(\"===> \",story)\n",
    "            print(\"===> \",story.string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목 : 의대증원 오늘 확정…전공의·의대생 복귀 가능성 더 멀어졌다\n",
      "작성자 : 천선휴 기자\n",
      "작성날짜와 시간 : 2024. 5. 24. 10:48\n",
      "첫번째 문단 (서울=뉴스1) 천선휴 기자 = 2025학년도 의과대학 정원이 24일 확정된다. 1999년 이후 27년 만의 의대정원 증원이다. \n",
      "(서울=뉴스1) 천선휴 기자 = 2025학년도 의과대학 정원이 24일 확정된다. 1999년 이후 27년 만의 의대정원 증원이다. \n",
      "하지만 이로써 학교를 떠나 있는 의대생들과 병원을 이탈한 전공의들이 돌아올 가능성도 더 멀어졌다. \n",
      "한국대학교육협의회는 24일 오후 제2차 대학입학전형위원회를 열고 의대 증원안이 포함된 대입전형 시행계획 변경안을 심의·확정한다. 이에 대한 심의 결과는 30일에, 대학별 모집요강은 31일에 발표된다. 이달 말이면 의대 증원 절차가 모두 마무리되는 것이다.\n",
      "일부 국립대에서는 의대 정원 증원을 확정하기 위해 필요한 학칙 개정이 부결되는 등 학내 갈등이 이어지고 있지만, 그럼에도 2025학년도 대입 전형과 모집정원은 그대로 확정된다는 게 교육부의 설명이다.\n",
      "이로써 내년 전국 40개 의과대학은 전년보다 1509명이 늘어난 4567명의 신입생을 뽑게 된다. 동시에 '원점 재검토'를 복귀의 전제로 주장해오던 전공의와 의대생들이 돌아올 명분도 사라졌다. \n",
      "전공의들이 끝내 돌아오지 않으면 당장 내년에 전문의 2910명이 배출되지 못한다. 이들 중엔 필수의료과에서 수련하던 전공의 1385명도 포함돼 있다. 가뜩이나 부족한 필수과 전문의들마저 1400여 명이 배출되지 않는다는 것이다. \n",
      "전문의 배출이 되지 않을 경우 군의관, 공중보건의 모집에도 영향을 미칠 수밖에 없다.\n",
      "다만 의료계에선 전문의 시험만 남겨둔 레지던트 4년 차(3년제 진료과목은 3년 차) 전공의들은 복귀해 시험을 치를 것이라는 전망도 내놓고 있다. 하지만 대세를 뒤흔들 만큼의 수가 아닌 극소수에 불과할 것으로 보고 있다. \n",
      "의대생들이 돌아올 확률은 더욱 적어 보인다. 전공의들은 가정을 꾸리거나 외벌이인 경우가 많아 생계를 이유로 돌아오길 고민하는 경우도 일부 있다고는 하지만 의대생들의 의지는 확고하다. 현재 의대생 1만 8348명 중 99.7%가 학교에 나오지 않고 있다. \n",
      "이들이 이 상태로 유급에 처해질 경우 현재 1학년인 3058명의 학생들과 내년 신입생 4567명까지 총 7625명이 함께 1학년 수업을 들어야 한다. 각 대학의 현재 시설과 인력으로는 감당할 수 없는 인원이다. \n",
      "또 의대생 졸업자가 나오지 않으면 병원들은 인턴도 뽑지 못하게 된다. \n",
      "문제는 각 대학의 교육 여건이 미진할 경우 한국의학교육평가원이 진행하는 평가에서 인증을 받지 못하게 돼 서남대 의대처럼 폐교를 하게 되는 일이 벌어질 수도 있다는 점이다. \n",
      "한 의과대학 교수는 \"엄청나게 늘어난 저 신입생들을 교육할 여력이 안 된다는 건 너무나 자명한 사실\"이라며 \"2025학년 의예과 1학년인 7625명은 6년간 학교를 같이 다녀야 하는데 향후 6년간 큰 부담을 줄 것\"이라고 말했다. \n",
      "또 이들이 졸업해 인턴을 지원할 때도 문제가 된다. 이 교수는 \"내년부터 늘어난 의대 정원이 졸업하는 2031년 7625명이 인턴을 지원하게 되는 것인데 병원이 얼마나 인턴 선발의 여력이 있을지 모르겠다\"며 \"결국 의대는 졸업하지만 인턴, 레지던트 과정을 밟을 수 없는 의사들은 어떻게 되는 것인가\"라고 우려했다. \n",
      "그러면서 \"대학병원들이 아직도 전공의가 들어오길 바라고, 전공의로 운영되는 진료시스템에 대해 미련을 가지고 있는데 이제 그런 상상은 2036년부터나 가능할 것\"이라고 말했다.\n",
      "정부는 비상진료체계 강화 대책을 잇따라 내놓으며 전공의 이탈에 따른 의료공백을 메우고 있지만 현실적으로 역부족인 상황이다. \n",
      "전공의들이 떠난 지 석달째 비상진료 체계를 운영하면서 공중보건의, 군의관을 파견하고 있지만 547명에 불과해 전공의 1만 3000여 명의 자리를 메우기엔 턱없이 부족하다. \n",
      "또 다른 대책으로 시니어 의사 채용 방안을 내놓고 지난달 16일 시니어 의사 지원센터를 개소했지만 문의 전화는 8건뿐인 데다 예산 확정 등도 되지 않아 초기 세팅 단계에 머무르고 있다.\n",
      "정부가 내세우고 있는 '전문의 중심 병원'도 당장 의료공백을 메울 수 없는 대책이긴 매한가지다. 병원 관계자들은 상급종합병원의 경우 전공의 비율이 40%에 달했는데 이들을 대신해 전문의를 채용하는 것은 불가능하다고 입을 모은다. \n",
      "이 같은 문제를 정부도 모르고 있는 것은 아니다. 조규홍 복지부 장관은 지난 22일 기자간담회에서 \"군의관, 공보의도 투입하고 간호사들의 진료지원도 확대하는 등 대책을 마련하고 있는데 이게 지속가능하진 않다\"며 \"하루빨리 전공의 돌아와서 자리를 메꿔주면 좋겠다\"고 말했다. \n",
      "그러면서 \"의료개혁 특별위원회를 빨리 가동하고 논의를 해서 의료개혁을 완성해가는 것이 정부의 대책\"이라고 덧붙였다. \n",
      "이에 정부는 이날 의개특위 산하 4개 전문위원회 중 의료인력 전문위원회의 첫 회의를 열고 전공의 연속 근무 시간 단축 등을 논의할 계획이다. 이날로 4개 전문위원회의 1차 회의가 마무리되는 것이다. \n",
      "이 전문위원회들은 앞으로 격주 회의를 열고 개혁 과제를 구체화해 나갈 방침이다.\n",
      "sssunhue@news1.kr \n"
     ]
    }
   ],
   "source": [
    "# select(), select_one() 으로 변경\n",
    "\n",
    "url = \"https://v.daum.net/v/20240524104849821\"\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url)\n",
    "    # print(r.text)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    # 뉴스 제목 \n",
    "    title = soup.select_one(\"h3.tit_view\")\n",
    "    print(f\"제목 : {title.text}\")\n",
    "\n",
    "    # 작성자 \n",
    "    writer = soup.select_one(\"span.txt_info\")\n",
    "    print(f\"작성자 : {writer.string}\")\n",
    "\n",
    "    # 작성날짜와 시간\n",
    "    num_date = soup.select_one(\"span.num_date\")\n",
    "    print(f\"작성날짜와 시간 : {num_date.string}\")\n",
    "\n",
    "    # 첫번째 문단 가져오기\n",
    "    para = soup.select_one(\"p[dmcf-ptype='general']\")\n",
    "    print(f\"첫번째 문단 {para.text}\")\n",
    "\n",
    "    # 전체 본문 내용 가져오기\n",
    "    paras = soup.select(\"p[dmcf-ptype='general']\")\n",
    "    # print(paras)\n",
    "    for p in paras:\n",
    "        print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML>\n",
      "<html lang=\"ko\">\n",
      "<head>\n",
      "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"/>\n",
      "    <meta name=\"viewport\" content=\"width=device-width,initial-scale=1.0,maximum-scale=1.0,minimum-scale=1.0,user-scalable=no\">\n",
      "    <meta name=\"description\" lang=\"ko\" content=\"잠시 후 다시 확인해주세요! : 네이버쇼핑\">\n",
      "    <title>에러 페이지 : 네이버쇼핑</title>\n",
      "    <link rel=\"stylesheet\" type=\"text/css\" href=\"//img.pay.naver.net/static/css/customer/naver_error.css\">\n",
      "\n",
      "    <script src=\"https://ssl.pstatic.net/static/fe/grafolio.js\"></script>\n",
      "</head>\n",
      "\n",
      "\n",
      "<body>\n",
      "<div id=\"u_skip\" class=\"u_skip\">\n",
      "    <a href=\"#content\">본문 바로가기</a>\n",
      "</div>\n",
      "<div class=\"wrap\">\n",
      "    <div class=\"header\" role=\"banner\">\n",
      "        <h1 class=\"logo\"><a href=\"//naver.com\" class=\"logo_link\"><img src=\"//img.pay.naver.net/static/images/customer/naver_logo.png\" width=\"90\" height=\"16\"\n",
      "                                                                             alt=\"네이버\"></a></h1>\n",
      "        <div class=\"nav\" role=\"navigation\">\n",
      "            <a href=\"//naver.com\" class=\"nav_link\">네이버홈</a>\n",
      "            <a href=\"//help.pay.naver.com\" class=\"nav_link\">쇼핑&페이 고객센터</a>\n",
      "        </div>\n",
      "    </div>\n",
      "    <hr>\n",
      "    <div class=\"container\" role=\"main\">\n",
      "        <div class=\"content\" id=\"content\">\n",
      "            <div class=\"image_area _errorImage\"></div>\n",
      "\n",
      "            <div class=\"info_area\">\n",
      "                <div class=\"info_txt\">\n",
      "                    <strong class=\"tit\">잠시 후 다시 확인해주세요!</strong>\n",
      "                    <p class=\"txt\">\n",
      "                        지금 이 서비스와 연결할 수 없습니다.<br>\n",
      "                        문제를 해결하기 위해 열심히 노력하고 있습니다.<br>\n",
      "                        잠시 후 다시 확인해주세요.\n",
      "                    </p>\n",
      "                </div>\n",
      "                <div class=\"info_link\">\n",
      "                    <a href=\"javascript:history.go(-1)\" class=\"link_prev\">이전 페이지</a><a href=\"//shopping.naver.com\" class=\"link_home\">네이버쇼핑 홈</a>\n",
      "                </div>\n",
      "            </div>\n",
      "        </div>\n",
      "    </div>\n",
      "    <hr>\n",
      "    <div class=\"footer\" role=\"contentinfo\">\n",
      "        <address>\n",
      "            <span>Copyright</span> ©<a href=\"http://www.navercorp.com\" class=\"link_naver\" target=\"_blank\">NAVER Corp.</a> <span>All Rights Reserved.</span>\n",
      "        </address>\n",
      "    </div>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "\n",
    "# url = \"https://shopping.naver.com/home\"\n",
    "url = \"https://shopping.naver.com/api/modules/gnb/category?id=root&_vc_=1717171131197\"\n",
    "\n",
    "userAgent = UserAgent()\n",
    "\n",
    "headers = {\"user-agent\":userAgent.chrome}\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url, headers=headers)\n",
    "    print(r.text)\n",
    "    #soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
